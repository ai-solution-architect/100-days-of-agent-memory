### ğƒğšğ² ğŸ/ğŸğŸğŸ ğ¨ğŸ ğ€ğ ğğ§ğ­ ğŒğğ¦ğ¨ğ«ğ²
[post](https://www.linkedin.com/posts/richmondalake_architecting-agent-memory-activity-7345776758417186816-V3Yp?utm_source=share&utm_medium=member_desktop&rcm=ACoAAAQRyLsBkSAisgCJmXavcznDzyDTxIecxnk)

Honestly, didn't know it would blow up like it did.
After just a couple of days, the talk I gave on Agent Memory at AI Engineer Conference has over 25k views in a few days. 

Not only that, but my little weekend side project on Agent Memory gained over 100 â­ last weekend.

So what's all the fuss about?
It turns out that many developers, practitioners, and founders are very interested in making AI Agents reliable, believable, and capable. And we all agree that memory is a core component in making this happen.

ğ–ğ¡ğšğ­'ğ¬ ğ€ğ ğğ§ğ­ ğŒğğ¦ğ¨ğ«ğ²?
AI agent memory is the persistent cognitive architecture that allows agents to accumulate knowledge, maintain contextual awareness, and adapt their behavior based on historical interactions and learned experiences. Think of it as giving AI a working memory that persists beyond a single interaction.

ğ–ğ¡ğ² ğğ¨ğğ¬ ğ€ğ ğğ§ğ­ ğŒğğ¦ğ¨ğ«ğ² ğ¦ğšğ­ğ­ğğ«?
As I said earlier, memory makes an AI agent reliable, believable, and capable. 
â†’ Without memory, every conversation with an AI agent starts from scratch.
â†’ Without memory, AI agents can't adapt to new information
â†’ Without memory, AI agents can't be personalized
â†’ Without memory, AGI won't happen

To learn in public and share the journey, I'm conducting a hashtag#ğŸğŸğŸğƒğšğ²ğ¬ğğŸğ€ğ ğğ§ğ­ğŒğğ¦ğ¨ğ«ğ² challenge starting today.

I'll be documenting my deep dive into key definitions, memory management techniques, research findings, implementation strategies, and real-world applications - sharing both insights and mistakes along the way.

Join me on this learning journey if you want to understand the backbone of truly intelligent AI systems.

Day 1 starts now. ğŸ§ 

---

### ğƒğšğ² ğŸ/ğŸğŸğŸ ğ¨ğŸ ğ€ğ ğğ§ğ­ ğŒğğ¦ğ¨ğ«ğ²
[post](https://www.linkedin.com/posts/richmondalake_moving-beyond-context-engineering-activity-7346263709477969920-6-7r?utm_source=share&utm_medium=member_desktop&rcm=ACoAAAQRyLsBkSAisgCJmXavcznDzyDTxIecxnk)
ğ…ğ«ğ¨ğ¦ ğ‚ğ¨ğ§ğ­ğğ±ğ­ ğ­ğ¨ ğŒğğ¦ğ¨ğ«ğ² ğ„ğ§ğ ğ¢ğ§ğğğ«ğ¢ğ§ğ 
Context Engineering is the buzzword of the week in AI circles, and for good reason. We're finally getting closer to the real work that needs to be done, but we need to go one step further to build truly reliable, believable, and capable AI agents.

ğ–ğ¡ğšğ­ ğ¢ğ¬ ğ‚ğ¨ğ§ğ­ğğ±ğ­ ğ„ğ§ğ ğ¢ğ§ğğğ«ğ¢ğ§ğ ?
Philipp Schmid describes it as "the discipline of designing and building dynamic systems that provides the right information and tools, in the right format, at the right time, to give a LLM everything it needs to accomplish a task."

Tobias LÃ¼tke, CEO of Shopify, captures the essence even more succinctly: "the art of providing all the context for the task to be plausibly solvable by the LLM."

And Andrej Karpathy blessed it.
So you know it's going in the Oxford dictionary along with Vibe Coding.

ğ“ğ¡ğ ğ‹ğ¢ğ¦ğ¢ğ­ğšğ­ğ¢ğ¨ğ§ ğ¨ğŸ ğ‚ğ¨ğ§ğ­ğğ±ğ­ ğ„ğ§ğ ğ¢ğ§ğğğ«ğ¢ğ§ğ 
ğ“ğ¡ğ ğŸğ®ğ§ğğšğ¦ğğ§ğ­ğšğ¥ ğ¥ğ¢ğ¦ğ¢ğ­ğšğ­ğ¢ğ¨ğ§: ğœğ¨ğ§ğ­ğğ±ğ­ ğğ§ğ ğ¢ğ§ğğğ«ğ¢ğ§ğ  ğ¨ğ©ğ­ğ¢ğ¦ğ¢ğ³ğğ¬ ğŸğ¨ğ« ğ¢ğ§ğğ¢ğ¯ğ¢ğğ®ğšğ¥ ğ¢ğ§ğ­ğğ«ğšğœğ­ğ¢ğ¨ğ§ğ¬, ğ›ğ®ğ­ ğ¢ğ­ ğğ¨ğğ¬ğ§'ğ­ ğğ§ğšğ›ğ¥ğ ğ¬ğ²ğ¬ğ­ğğ¦ğ¬ ğ­ğ¡ğšğ­ ğ¥ğğšğ«ğ§ ğšğ§ğ ğğ¯ğ¨ğ¥ğ¯ğ ğ¨ğ¯ğğ« ğ­ğ¢ğ¦ğ. Your AI agent handles a complex customer inquiry perfectly, but forgets everything about that interaction.

ğ„ğ§ğ­ğğ« ğŒğğ¦ğ¨ğ«ğ² ğ„ğ§ğ ğ¢ğ§ğğğ«ğ¢ğ§ğ 
Memory engineering addresses this by moving AI systems from stateless optimization to persistent intelligence.

Instead of just asking: 
âŒ What information does this system need right now?
Memory engineering asks: 
âœ… How can this system accumulate understanding that makes it more reliable and capable over time?

Here's what it looks like in practiceâ†´
A context-engineered customer service agent retrieves relevant policy documents and previous tickets to answer current questions. 
A memory-engineered system does all of that, plus remembers interaction patterns, learns which communication approaches work best with specific customers, and continuously refines its understanding of common problem resolution strategies.

ğŒğğ¦ğ¨ğ«ğ² ğğ§ğ ğ¢ğ§ğğğ«ğ¢ğ§ğ  ğ¢ğ¬ ğ­ğ¡ğ ğğ¢ğ¬ğœğ¢ğ©ğ¥ğ¢ğ§ğ ğ¨ğŸ ğœğ«ğğšğ­ğ¢ğ§ğ  ğ¬ğ²ğ¬ğ­ğğ¦ğ¬ ğ­ğ¡ğšğ­ ğ¬ğ²ğ¬ğ­ğğ¦ğšğ­ğ¢ğœğšğ¥ğ¥ğ² ğ©ğ«ğ¨ğœğğ¬ğ¬, ğ¬ğ­ğ¨ğ«ğ, ğ«ğğ­ğ«ğ¢ğğ¯ğ, ğšğ§ğ ğ®ğ©ğğšğ­ğ ğ¢ğ§ğŸğ¨ğ«ğ¦ğšğ­ğ¢ğ¨ğ§ ğ­ğ¨ ğ­ğ«ğšğ§ğ¬ğŸğ¨ğ«ğ¦ ğ«ğšğ° ğğšğ­ğš ğ¢ğ§ğ­ğ¨ ğ©ğğ«ğ¬ğ¢ğ¬ğ­ğğ§ğ­ ğ¦ğğ¦ğ¨ğ«ğ² ğœğ¨ğ¦ğ©ğ¨ğ§ğğ§ğ­ğ¬, ğ­ğ¡ğğ§ ğğ²ğ§ğšğ¦ğ¢ğœğšğ¥ğ¥ğ² ğ«ğğ­ğ«ğ¢ğğ¯ğ ğšğ§ğ ğšğ¬ğ¬ğğ¦ğ›ğ¥ğ ğ­ğ¡ğ ğ¦ğ¨ğ¬ğ­ ğ«ğğ¥ğğ¯ğšğ§ğ­ ğ¦ğğ¦ğ¨ğ«ğ² ğœğ¨ğ¦ğ©ğ¨ğ§ğğ§ğ­ğ¬ ğ­ğ¨ ğ©ğ¨ğ©ğ®ğ¥ğšğ­ğ ğšğ§ ğ‹ğ‹ğŒ'ğ¬ ğœğ¨ğ§ğ­ğğ±ğ­ ğ°ğ¢ğ§ğğ¨ğ° ğŸğ¨ğ« ğğšğœğ¡ ğ¢ğ§ğ­ğğ«ğšğœğ­ğ¢ğ¨ğ§ ğ¨ğ« ğ¬ğğ¬ğ¬ğ¢ğ¨ğ§.

To be clear, Memory engineering doesn't replace context engineering
It builds on it.

hashtag#ğŸğŸğŸğƒğšğ²ğ¬ğğŸğ€ğ ğğ§ğ­ğŒğğ¦ğ¨ğ«ğ² hashtag#ğ€ğˆ hashtag#ğŒğğ¦ğ¨ğ«ğ²ğ„ğ§ğ ğ¢ğ§ğğğ«ğ¢ğ§ğ  hashtag#ğ€ğ ğğ§ğ­ğŒğğ¦ğ¨ğ«ğ²

---

### ğƒğšğ² ğŸ‘/ğŸğŸğŸ ğ¨ğŸ ğ€ğ ğğ§ğ­ ğŒğğ¦ğ¨ğ«ğ²

[post](https://www.linkedin.com/posts/richmondalake_100daysofagentmemory-ai-agentmemory-activity-7346687979605647360-lRaR?utm_source=share&utm_medium=member_desktop&rcm=ACoAAAQRyLsBkSAisgCJmXavcznDzyDTxIecxnk)
Let's get you explaining Agent Memory like a pro.

Before we dive deeper into Agent Memory
It helps to understand why it matters in the first place. 
Whenever I'm asked about the importance of memory in AI 

ğˆ ğğ±ğ©ğ¥ğšğ¢ğ§ ğ¢ğ­ ğ­ğ¡ğ¢ğ¬ ğ°ğšğ²:
If the aim of artificial intelligence is to mimic human intelligence, and one of the core components of intelligence is the ability to retain, recall, and reuse informationâ€”which in humans is governed by memoryâ€”then you can make the argument that for AI to achieve its objective, it needs some form of augmented computational memory.

Today, the agreed-upon form factor of AI is predominantly AI Agents and agentic systems.

ğ–ğ¡ğ² ğŒğğ¦ğ¨ğ«ğ² ğ¢ğ¬ ğğ¨ğ§-ğğğ ğ¨ğ­ğ¢ğšğ›ğ¥ğ
The importance of memory becomes crystal clear when you understand that it's the core component that makes AI agents more Reliable, Believable, and Capableâ€”I call this the RBCs of AI Agents.
1ï¸âƒ£ Reliable through consistent access to relevant historical context and learned patterns
2ï¸âƒ£ Believable through coherent behavior that reflects accumulated experiences
3ï¸âƒ£ Capable through the ability to leverage past knowledge for complex reasoning

ğŒğğ¦ğ¨ğ«ğ² ğ“ğ²ğ©ğğ¬ ğğ¨ğ°ğğ«ğ¢ğ§ğ  ğ‘ğğ‚ ğ€ğ ğğ§ğ­ğ¬
Different memory types enable different aspects of RBC behavior:
1ï¸âƒ£ Reliability is powered by Episodic Memory - storing specific interactions with timestamps and context, ensuring agents can consistently reference past events and maintain coherent decision-making patterns over time.
2ï¸âƒ£ Believability emerges from Semantic Memory - accumulating general knowledge and relationships that enable agents to demonstrate understanding that feels natural and contextually appropriate, just like humans draw from learned concepts. Persona memory also plays a role here, more on this later.
3ï¸âƒ£ Capability is enhanced by Procedural Memory - retaining learned skills, workflows, and problem-solving approaches that allow agents to execute complex tasks more effectively through accumulated experience.

ğ€ ğ‹ğğšğ«ğ§ğ¢ğ§ğ  ğ„ğ±ğ©ğ¥ğ¨ğ«ğšğ­ğ¢ğ¨ğ§ ğ¢ğ§ ğ‘ğğšğ¥ ğ“ğ¢ğ¦ğ
Full transparency: I'm not an expert teaching unsettled science. 
I'm an engineer actively exploring and uncovering these concepts alongside the broader AI community. 

So far, I find that this journey is as much about asking the right questions as it is about finding answers.

hashtag#ğŸğŸğŸğƒğšğ²ğ¬ğğŸğ€ğ ğğ§ğ­ğŒğğ¦ğ¨ğ«ğ² hashtag#ğ€ğˆ hashtag#ğ€ğ ğğ§ğ­ğŒğğ¦ğ¨ğ«ğ² hashtag#ğ€ğˆğ€ğ ğğ§ğ­ğ¬

---

### ğƒğšğ² ğŸ’/ğŸğŸğŸ ğ¨ğŸ ğ€ğ ğğ§ğ­ ğŒğğ¦ğ¨ğ«ğ²

[post](https://www.linkedin.com/posts/richmondalake_agent-memory-application-mode-activity-7347050219009695744-jaVE?utm_source=share&utm_medium=member_desktop&rcm=ACoAAAQRyLsBkSAisgCJmXavcznDzyDTxIecxnk)
Let's talk about Application mode 
Their importance and how they map to Memory Types.

I've been thinking a lot about how the application mode an agent is expected to run in fundamentally defines the memory architecture it needs.

ğ–ğ¡ğšğ­ ğ¢ğ¬ ğ€ğ©ğ©ğ¥ğ¢ğœğšğ­ğ¢ğ¨ğ§ ğŒğ¨ğğ?
Application mode refers to the operational environment and interaction patterns an agent is designed for. It's not about the domain (legal vs. medical vs. coding) but about how the agent fundamentally operates.
The three core application modes I'm observing are:
1ï¸âƒ£ Assistant - Task-oriented systems providing specialized help through interactive dialogue while maintaining user context
2ï¸âƒ£ Workflow - Process-driven systems that orchestrate multi-step operations through structured execution paths and tool coordination
3ï¸âƒ£ Deep Research - Multi-source investigation systems that gather, synthesize, and report findings over extended periods

ğ–ğ¡ğ² ğ€ğ©ğ©ğ¥ğ¢ğœğšğ­ğ¢ğ¨ğ§ ğŒğ¨ğğ ğŒğšğ­ğ­ğğ«ğ¬
Identifying the right application mode when designing agent memory affects downstream decision in your AI stack:
âœ… Embedding models - General purpose vs. domain-specialized 
âœ… Memory architecture - Which memory types your agent needs 
âœ… Agentic architecture - Single agent vs. multi-agent orchestration
âœ… Retrieval methods - How your memory provider should be optimized

For example, Deep Research mode typically requires multi-agent architecture due to long-running execution and query parallelization. You spawn multiple agents for different research aspects, consolidate findings in shared memory, and have an orchestrator synthesize the final report.

When exploring the topic of application mode in agent memory I made I mistake. I viewed Coding, as a mode, which is wrong, it's instead a subset of Assistant Mode. I realized this when I making some changes to MemoRizz and trying to consolidate patterns in how agents are initialized.

ğŸ’¡So, the insight here is coding assistants, legal assistants, and medical assistants are all domain-specific variants of Assistant modeâ€”they share the same operational patterns and memory requirements, just with different specialized knowledge.

Check out the attached PDF for more info.
And let me know what you think of Application Mode.

hashtag#ğŸğŸğŸğƒğšğ²ğ¬ğğŸğ€ğ ğğ§ğ­ğŒğğ¦ğ¨ğ«ğ² hashtag#ğ€ğˆ hashtag#ğ€ğ ğğ§ğ­ğŒğğ¦ğ¨ğ«ğ² hashtag#ğ€ğˆğ€ğ ğğ§ğ­ğ¬

---

### ğƒğšğ² ğŸ“/ğŸğŸğŸ ğ¨ğŸ ğ€ğ ğğ§ğ­ ğŒğğ¦ğ¨ğ«ğ²

[post](https://www.linkedin.com/posts/richmondalake_100daysofagentmemory-100daysofagentmemory-activity-7347420372973084672-BiSo?utm_source=share&utm_medium=member_desktop&rcm=ACoAAAQRyLsBkSAisgCJmXavcznDzyDTxIecxnk)
The best way to learn is to build.
So I'm building MemoRizz.

Five days into this journey, and I'm already exploring some fascinating research about agent memory architectures. But reading papers, articles and theorizing only gets you so far. To truly understand how memory works in AI agents, you need to get your hands dirty and built

ğˆğ§ğ­ğ«ğ¨ğğ®ğœğ¢ğ§ğ  ğŒğğ¦ğ¨ğ‘ğ¢ğ³ğ³
I'm building an open-source Python library that serves as a memory layer for AI applications. Think of it as the practical implementation of everything I'm learning in this hashtag#100DaysOfAgentMemory series.
I will probably get a lot of things(definitions, techniques and implementations) wrong, and I think that's fine.

ğ‹ğğšğ«ğ§ğ¢ğ§ğ  ğ¢ğ§ ğğ®ğ›ğ¥ğ¢ğœ, ğğ®ğ¢ğ¥ğğ¢ğ§ğ  ğ¢ğ§ ğğ©ğğ§
The point is that most of the concept I explore in this series gets tested, implemented, and refined in MemoRizz. When I discover something interesting about how procedural memory should work in workflow agents, it goes straight into the codebase.

On day 5ï¸âƒ£ I worked on a form of episodic memories that some refer to as summarisation or observations. This is where you consolidate logs of interactions within a session, and compress them into summaries that can be used in future execution of agents for broader context of a problem domain or environment. A question I asked myself is when do I trigger a summarisation? When the context window gets full ? At the end of the session or day?

Either way I will release this feature in a few days in MemoRizz.

ğ–ğ¡ğ² ğğ©ğğ§ ğ’ğ¨ğ®ğ«ğœğ?
Agent memory is still an emerging field, and I believe the breakthroughs will come from the communityâ€”researchers, engineers, and builders working together to solve these fundamental challenges.

ğ–ğ¡ğšğ­'ğ¬ ğğğ±ğ­?
Over the next 95 days, MemoRizz will evolve alongside my understanding. 
Each post in this series will likely spawn new features, optimizations, or architectural decisions in the library. 
It's going to be messy and iterative, but we will a lot along the way.

The library is already gaining traction
â€”322 stars and growing
â€”which tells me we're onto something important here.

Want to contribute? 
Check out the repo: https://lnkd.in/eNwAZRhf

hashtag#ğŸğŸğŸğƒğšğ²ğ¬ğğŸğ€ğ ğğ§ğ­ğŒğğ¦ğ¨ğ«ğ² hashtag#ğğ©ğğ§ğ’ğ¨ğ®ğ«ğœğ hashtag#ğ€ğˆ hashtag#ğ€ğ ğğ§ğ­ğŒğğ¦ğ¨ğ«ğ² hashtag#ğğ²ğ­ğ¡ğ¨ğ§

---


